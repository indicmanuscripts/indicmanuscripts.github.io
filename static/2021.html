<!DOCTYPE html>
<!-- saved from url=(0027)https://learn3dg.github.io/ -->
<html><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">

  <meta name="keywords" content="cvpr, workshop, computer vision, computer graphics, visual learning, simulation environments, robotics, machine learning, natural language processing, reinforcement learning">

  <link rel="shortcut icon" href="https://learn3dg.github.io/static/img/ico/favicon.png">



  <title>Learning to Generate 3D Shapes and Scenes</title>
  <meta name="description" content="Website for the Workshop on Learning to Generate 3D Shapes and Scenes at CVPR 2021 ---">

  <!--Open Graph Related Stuff-->
  <meta property="og:title" content="Learning to Generate 3D Shapes and Scenes">
  <meta property="og:url" content="https://learn3dg.github.io/">
  <meta property="og:description" content="Website for the Workshop on Learning to Generate 3D Shapes and Scenes at CVPR 2021 ---">
  <meta property="og:site_name" content="Learning to Generate 3D Shapes and Scenes">
  <meta property="og:image" content="">
  <meta property="og:image:url" content="">

  <!--Twitter Card Stuff-->
  <meta name="twitter:card" content="summary_large_image">
  <meta name="twitter:title" content="Learning to Generate 3D Shapes and Scenes">
  <meta name="twitter:image" content="https://learn3dg.github.io/static/img/bg.png">
  <meta name="twitter:url" content="https://learn3dg.github.io">
  <meta name="twitter:description" content="Website for the Workshop on Learning to Generate 3D Shapes and Scenes at CVPR 2021 ---">

  <!-- CSS  -->
  <link rel="stylesheet" type="text/css" href="./2021_files/bootstrap.min.css">
  <link rel="stylesheet" type="text/css" href="./2021_files/main.css" media="screen,projection">
</head>

  <body style="">

    <!-- <div class="top-strip"></div> -->
<div class="navbar navbar-default navbar-fixed-top">
  <div class="container">
    
    <div class="navbar-header">
      <a class="navbar-brand" href="https://learn3dg.github.io/"></a>
      <button class="navbar-toggle" type="button" data-toggle="collapse" data-target="#navbar-main">
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
    </div>

    <div class="navbar-collapse collapse" id="navbar-main">
      <ul class="nav navbar-nav">
        <li><a href="https://learn3dg.github.io/#intro">Introduction</a></li>
        <li><a href="https://learn3dg.github.io/#cfp">Call for Papers</a></li>
        <li><a href="https://learn3dg.github.io/#dates">Important Dates</a></li>
        <li><a href="https://learn3dg.github.io/#schedule">Schedule</a></li>
        <li><a href="https://learn3dg.github.io/#accepted">Accepted Papers</a></li>
        <li><a href="https://learn3dg.github.io/#speakers">Invited Speakers</a></li>
        <li><a href="https://learn3dg.github.io/#organizers">Organizers</a></li>
      </ul>
    </div>

  </div>
</div>


    <div class="container">
      <div class="page-content">
          <p><br></p>
<div class="row">
  <div class="col-xs-12">
    <center><h1>Learning to Generate 3D Shapes and Scenes</h1></center>
    <center><h2>CVPR 2021 Workshop</h2></center>
    <center><span style="font-weight:400;">June 25th 2021 AM CDT</span></center>
    <center><span style="color:#e74c3c;font-weight:400;"></span></center>
    <br>
  </div>
</div>

<hr>

<p><b>Join YouTube live stream at <a href="https://youtu.be/fEPYHLXOZKQ">https://youtu.be/fEPYHLXOZKQ</a>.</b></p>

<p><b>Submit questions for the closing panel discussion using this google form: <a href="https://forms.gle/G5JCiKA2MmLogMiA9">https://forms.gle/G5JCiKA2MmLogMiA9</a>.</b></p>

<p><b>We would like to get your thoughts and opinions on academic datasets for learning to generate 3D shapes and 3D scenes. Please fill out this short survey: <a href="https://forms.gle/F9EtS4RCraoLtmwA6">https://forms.gle/F9EtS4RCraoLtmwA6</a>.</b></p>

<!-- <b>Please give us your feedback on how the workshop went using this Google form: <a href='https://forms.gle/utMpEnF4hmUcR19S7'>https://forms.gle/utMpEnF4hmUcR19S7</a>.</b> -->

<div class="row" id="intro">
  <div class="col-xs-12">
    <h2>Introduction</h2>
  </div>
</div>
<div class="row">
  <div class="col-xs-12">
    <p>
     This workshop aims to bring together researchers working on generative models of 3D shapes and scenes with researchers and practitioners who use these generative models in a variety of research areas. For our purposes, we define "generative model" to include methods that synthesize geometry unconditionally as well as from sensory inputs (e.g. images), language, or other high-level specifications. Vision tasks that can benefit from such models include scene classification and segmentation, 3D reconstruction, human activity recognition, robotic visual navigation, question answering, and more.
    </p>
  </div>
</div>
<p><br></p>

<div class="row" id="schedule">
  <div class="col-xs-12">
    <h2>Schedule</h2>
    <p>All times in CDT Central Daylight Time (UTC-05:00)</p>
  </div>
</div>

<div class="row">
  <div class="col-xs-12">
     <table class="table table-striped">
      <tbody>
        <tr>
          <td>9:00am - 9:15am</td>
          <td>Welcome and Introduction</td>
          <td></td>
        </tr>
        <tr>
          <td>9:15am - 9:40am</td>
          <td>
          Invited Talk 1 (Rana Hanocka)
          <br>
          <i>Title: Neural 3D Reconstruction</i>
          </td>
          <td></td>
        </tr>
        <tr>
          <td>9:40am - 10:05am</td>
          <td>
          Invited Talk 2 (S.M. Ali Eslami)
          <br>
          <i>Title: Priors, Representation and Rendering for 3D Vision</i>
          </td>
          <td></td>
        </tr>
        <tr>
          <td>10:05am - 10:30am</td>
          <td>Invited Talk 3 (Kai (Kevin) Xu)
          <br>
          <i>Title: Deep Hierarchical Models for 3D Shape Understanding and Generation</i>
          </td>
          <td></td>
        </tr>
        <tr>
          <td>10:30am - 11:25am</td>
          <td>Paper Spotlight Talks</td>
          <td></td>
        </tr>
        <!-- <tr>
          <td>11:10am - 11:25am</td>
          <td>Break</td>
          <td></td>
        </tr> -->
        <tr>
          <td>11:25am - 11:50am</td>
          <td>
          Invited Talk 4 (Katerina Fragkiadaki)
          <br>
          <i>Title: Learning to see by predicting views with  3D neural feature bottleneck networks</i>
          </td>
          <td></td>
        </tr>
        <tr>
          <td>11:50am - 12:15pm</td>
          <td>Invited Talk 5 (Roozbeh Mottaghi)
          <br>
          <i>Title: Learning Representations via Interaction with the 3D World</i>
          </td>
          <td></td>
        </tr>
        <tr>
          <td>12:15pm - 1:00pm</td>
          <td>Panel Discussion (speakers &amp; panelists)</td>
          <td></td>
        </tr>
      </tbody>
    </table>
  </div>
</div>

<div class="row" id="speakers">
  <div class="col-xs-12">
    <h2>Invited Speakers &amp; Panelists</h2>
  </div>
</div>
<p><br></p>

<div class="row">
  <div class="col-md-12">
    <a href="https://www.cs.cmu.edu/~katef/"><img class="people-pic" style="float:left;margin-right:50px;" src="./2021_files/katerina.png"></a>
    <p>
      <b><a href="https://www.cs.cmu.edu/~katef/">Katerina Fragkiadaki</a></b> is an Assistant Professor in the Machine Learning Department at Carnegie Mellon. Prior to joining MLD's faculty she worked as a post doctoral researcher first at UC Berkeley working with Jitendra Malik and then at Google Research in Mountain View working with the video group. Katerina is interested in building machines that understand the stories that videos portray, and, inversely, in using videos to teach machines about the world. The pen-ultimate goal is to build a machine that understands movie plots, and the ultimate goal is to build a machine that would want to watch Bergman over this.
    </p>
  </div>
</div>
<p><br></p>

<div class="row">
  <div class="col-md-12">
    <a href="https://www.cs.tau.ac.il/~hanocka/"><img class="people-pic" style="float:left;margin-right:50px;" src="./2021_files/rana.png"></a>
    <p>
      <b><a href="https://www.cs.tau.ac.il/~hanocka/">Rana Hanocka</a></b> is an Assistant Professor of Computer Science at the University of Chicago. She received her Ph.D. under the supervision of Daniel Cohen-Or and Raja Giryes at Tel Aviv University. She is interested in the combination of computer graphics and machine learning. Specifically, she is interested in using deep learning and exploring neural representations for manipulating, analyzing, and understanding 3D shapes.
    </p>
  </div>
</div>
<p><br></p>

<div class="row">
  <div class="col-md-12">
    <a href="https://roozbehm.info/"><img class="people-pic" style="float:left;margin-right:50px;" src="./2021_files/roozbeh.png"></a>
    <p>
      <b><a href="https://roozbehm.info/">Roozbeh Mottaghi</a></b> is the Research Manager of the PRIOR team at Allen Institute for AI and an Affiliate Associate Professor in Paul G. Allen School of Computer  Science &amp; Engineering at the  University  of  Washington. Prior to joining AI2, he was a post-doctoral researcher at the Computer Science Department at Stanford University. He obtained his PhD in Computer Science in 2013from University of California, Los Angeles.  His research is mainly focused on Computer Vision and Machine Learning.
    </p>
  </div>
</div>
<p><br></p>

<div class="row">
  <div class="col-md-12">
    <a href="http://arkitus.com/research/"><img class="people-pic" style="float:left;margin-right:50px;" src="./2021_files/ali.png"></a>
    <p>
      <b><a href="http://arkitus.com/research/">S. M. Ali Eslami</a></b> is a Staff Research Scientist at Google DeepMind working on problems related to artificial intelligence. His group's research is focused on figuring out how we can get computers to learn with less supervision. Previously he was a post-doctoral researcher at Microsoft Research Cambridge. He did his PhD at the University of Edinburgh, where he was a Carnegie scholar working with Christopher Williams. During his PhD, he was also a visiting researcher at Oxford University working with Andrew Zisserman.
    </p>
  </div>
</div>
<p><br></p>

<div class="row">
  <div class="col-md-12">
    <a href="https://kevinkaixu.net/"><img class="people-pic" style="float:left;margin-right:50px;" src="./2021_files/kevin.png"></a>
    <p>
      <b><a href="https://kevinkaixu.net/">Kai (Kevin) Xu</a></b> is an Associate Professor at the School of Computer Science, National University of Defense Technology, where he received his PhD in 2011. He conducted visiting research at Simon Fraser University (2008-2010) and Princeton University (2017-2018). His research interests include geometry processing and geometric modeling, especially on data-driven approaches to the problems in those directions, as well as 3D geometry-based vision and its robotic applications. He has co-organized several courses and tutorials on those topics at prestigious venues such as SIGGRAPH and Eurographics.
    </p>
  </div>
</div>
<p><br></p>

<div class="row">
  <div class="col-md-12">
    <a href="https://www.cs.utexas.edu/~huangqx/"><img class="people-pic" style="float:left;margin-right:50px;" src="./2021_files/qixing.png"></a>
    <p>
      <b><a href="https://www.cs.utexas.edu/~huangqx/">Qixing Huang</a></b> is an Assistant Professor of Computer Science at the University of Texas at Austin. He obtained his PhD in Computer Science from Stanford University in 2012. From 2012 to 2014 he was a postdoctoral research scholar at Stanford University. From 2014 to 2016 he was a Research Assistant Professor at Toyota Technological Institute at Chicago. He received his MS and BS in Computer Science from Tsinghua University. He has also interned at Google Street View, Google Research and Adobe Research. His research spans computer vision, computer graphics, computational biology and machine learning.
    </p>
  </div>
</div>
<p><br></p>

<div class="row">
  <div class="col-md-12">
    <a href="https://www2.cs.sfu.ca/~haoz/"><img class="people-pic" style="float:left;margin-right:50px;" src="./2021_files/richard.png"></a>
    <p>
      <b><a href="https://www2.cs.sfu.ca/~haoz/">Hao (Richard) Zhang</a></b> is a Distinguished University Professor at Simon Fraser University. He obtained his PhD from the University of Toronto, and M.Math and B.Math degrees from Waterloo. His research is in computer graphics with special interests in geometric modeling, shape analysis, 3D vision, geometric deep learning, as well as computational design and fabrication. He has published more than 150 papers on these topics and methods from three of his papers on geometry processing have been adopted by CGAL, the open-source Computational Geometry Algorithms Library. Awards won by Richard include an NSERC Discovery accelerator Supplement Award in 2014, a Google Faculty Research Award in 2019, as well as faculty grants/gifts from Adobe and Autodesk. He and his students have won the CVPR 2020 Best Student Paper Award and best paper awards at SGP 2008 and CAD/Graphics 2017.
    </p>
  </div>
</div>
<p><br></p>

<div class="row" id="cfp">
  <div class="col-xs-12">
    <h2>Call for Papers</h2>
  </div>
</div>
<div class="row">
  <div class="col-xs-12">
    <p>
      <span style="font-weight:500;">Call for papers:</span> We invite papers of up to 8 pages for work on tasks related to data-driven 3D generative modeling or tasks leveraging generated 3D content.
      Paper topics may include but are not limited to:
    </p>
    <ul>
      <li>Generative models for 3D shape and 3D scene synthesis</li>
      <li>Generating 3D shapes and scenes from real world data (images, videos, or scans)</li>
      <li>Representations for 3D shapes and scenes</li>
      <li>Completion of 3D scenes or objects in 3D scenes</li>
      <li>Unsupervised feature learning for vision tasks via 3D generative models</li>
      <li>Training data synthesis/augmentation for vision tasks via 3D generative models</li>
    </ul>
    <p>
      <span style="font-weight:500;">Submission:</span> we encourage submissions of up to 8 pages excluding references and acknowledgements.
      The submission should be in the CVPR format.
      Reviewing will be single blind.
      Accepted papers will be made publicly available as non-archival reports, allowing future submissions to archival conferences or journals.
      We welcome already published papers that are within the scope of the workshop (without re-formatting), including papers from the main CVPR conference.
      Please submit your paper to the following address by the deadline: <span style="color:#1a1aff;font-weight:400;"><a href="mailto:3dscenegeneration@gmail.com">3dscenegeneration@gmail.com</a></span>
      Please mention in your email if your submission has already been accepted for publication (and the name of the conference).
    </p>
  </div>
</div>

<div class="row" id="dates">
  <div class="col-xs-12">
    <h2>Important Dates</h2>
  </div>
</div>

<div class="row">
  <div class="col-xs-12">
    <table class="table table-striped">
      <tbody>
        <tr>
          <td>Paper Submission Deadline</td>
          <td>May 24 2021 - AoE time (UTC -12)</td>
        </tr>
        <tr>
          <td>Notification to Authors</td>
          <td>May 31 2021</td>
        </tr>
        <tr>
          <td>Camera-Ready Deadline</td>
          <td>June 7 2021</td>
        </tr>
        <tr>
          <td>Workshop Date</td>
          <td>June 25 2021</td>
        </tr>
      </tbody>
    </table>
  </div>
</div>

<p><br></p>
<div class="row" id="accepted">
  <div class="col-md-12">
    <h2>Accepted Papers</h2>
  </div>
</div>

<div class="row text-center">
  <div class="col-md-12">
    <hr><br>
    <span style="font-weight:bold;">
    <img src="./2021_files/0001-poster.png" width="700"><br>
    <a href="https://drive.google.com/file/d/1ugipZxc2kSe1OtqVCdJTEGYUwQmtMpXK/view?usp=sharing">From Real to Synthetic and Back: Synthesizing Training Data for Multi-Person Scene Understanding</a></span>
    <br>
    <i>Igor Kviatkovsky, Nadav Bhonker, Gerard Medioni</i>
    <br>
    <a href="https://drive.google.com/file/d/1ugipZxc2kSe1OtqVCdJTEGYUwQmtMpXK/view?usp=sharing">Paper</a> | <a href="https://drive.google.com/file/d/16DBkUu5PAWNPfIUJJMdze1DN8zdWFB9n/view?usp=sharing">Poster</a> | <a href="https://learn3dg.github.io/">Video (available after workshop)</a>
    <br><hr><br>
    <span style="font-weight:bold;">
    <img src="./2021_files/0002-poster.png" width="700"><br>
    <a href="https://drive.google.com/file/d/1FrTPoG0U-q7nCtL0oQhlQCBeCxjSUdh5/view?usp=sharing">Deep Mesh Prior: Unsupervised Mesh Restoration using Graph Convolutional Networks</a></span>
    <br>
    <i>Shota Hattori, Tatsuya Yatagawa, Yutaka Ohtake, Hiromasa Suzuki</i>
    <br>
    <a href="https://drive.google.com/file/d/1FrTPoG0U-q7nCtL0oQhlQCBeCxjSUdh5/view?usp=sharing">Paper</a> | <a href="https://drive.google.com/file/d/1Wv_KR7P3OBXdyB4OdvHymUCxO1GvJfEZ/view?usp=sharing">Poster</a> | <a href="https://learn3dg.github.io/">Video (available after workshop)</a>
    <br><hr><br>
    <span style="font-weight:bold;">
    <img src="./2021_files/0003-poster.png" width="700"><br>
    <a href="https://drive.google.com/file/d/1TnK_pJ8qbyut0qhwyXLS0CJOV8ke5fWi/view?usp=sharing">Adaptive Multiplane Image Generation from a Single Internet Picture</a></span>
    <br>
    <i>Diogo C. Luvizon, Gustavo Sutter P. Carvalho, Andreza A. dos Santos, Jhonatas S. Conceicao,<br>Jose L. Flores-Campana, Luis G. L. Decker, Marcos R. Souza, Helio Pedrini, Antonio Joia, Otavio A. B. Penatti</i>
    <br>
    <a href="https://drive.google.com/file/d/1TnK_pJ8qbyut0qhwyXLS0CJOV8ke5fWi/view?usp=sharing">Paper</a> | <a href="https://drive.google.com/file/d/1v1Hy30gLrszxVYg1vpcr1W6fqGNsukD_/view?usp=sharing">Poster</a> | <a href="https://learn3dg.github.io/">Video (available after workshop)</a>
    <br><hr><br>
    <span style="font-weight:bold;">
    <img src="./2021_files/0004-poster.png" width="700"><br>
    <a href="https://drive.google.com/file/d/15Ln64WaRAsFGcoNX4H09LWHGrn0IIWiq/view?usp=sharing">Unconstrained Scene Generation with Locally Conditioned Radiance Fields</a></span>
    <br>
    <i>Terrance DeVries, Miguel Angel Bautista, Nitish Srivastava, Graham W. Taylor, Joshua M. Susskind</i>
    <br>
    <a href="https://drive.google.com/file/d/15Ln64WaRAsFGcoNX4H09LWHGrn0IIWiq/view?usp=sharing">Paper</a> | <a href="https://drive.google.com/file/d/15Ln64WaRAsFGcoNX4H09LWHGrn0IIWiq/view?usp=sharing">Poster</a> | <a href="https://learn3dg.github.io/">Video (available after workshop)</a>
    <br><hr><br>
    <span style="font-weight:bold;">
    <img src="./2021_files/0005-poster.png" width="700"><br>
    <a href="https://drive.google.com/file/d/1pWGNahqRT_s_vNL69rs5TYJRB16MnJre/view?usp=sharing">Rethinking Sampling in 3D Point Cloud Generative Adversarial Networks</a></span>
    <br>
    <i>He Wang, Zetian Jiang, Li Yi, Kaichun Mo, Hao Su, Leonidas Guibas</i>
    <br>
    <a href="https://drive.google.com/file/d/1pWGNahqRT_s_vNL69rs5TYJRB16MnJre/view?usp=sharing">Paper</a> | <a href="https://drive.google.com/file/d/1RF77Zp6cQgoU0tf33Wjthx0D6tr3IYAJ/view?usp=sharing">Poster</a> | <a href="https://learn3dg.github.io/">Video (available after workshop)</a>
    <br><hr>
  </div>
</div>

<div class="row" id="organizers">
  <div class="col-xs-12">
    <h2>Organizers</h2>
  </div>
</div>

<div class="row text-center">
  <div class="col-xs-2">
    <a href="https://manyili12345.github.io/">
      <img class="people-pic" src="./2021_files/manyi.png">
    </a>
    <div class="people-name">
      <a href="https://manyili12345.github.io/">Manyi Li</a>
      <h6>Simon Fraser University</h6>
    </div>
  </div>

  <div class="col-xs-2">
    <a href="https://www.cs.utexas.edu/~yzp12/">
      <img class="people-pic" src="./2021_files/zhenpei.png">
    </a>
    <div class="people-name">
      <a href="https://www.cs.utexas.edu/~yzp12/">Zhenpei Yang</a>
      <h6>UT Austin</h6>
    </div>
  </div>

  <div class="col-xs-2">
    <a href="https://angelxuanchang.github.io/">
      <img class="people-pic" src="./2021_files/angel.png">
    </a>
    <div class="people-name">
      <a href="https://angelxuanchang.github.io/">Angel X. Chang</a>
      <h6>Simon Fraser University</h6>
    </div>
  </div>

  <div class="col-xs-2">
    <a href="https://www.cse.iitb.ac.in/~sidch/">
      <img class="people-pic" src="./2021_files/sid.png">
    </a>
    <div class="people-name">
      <a href="https://www.cse.iitb.ac.in/~sidch/">Siddhartha Chaudhuri</a>
      <h6>Adobe Research, IIT Bombay</h6>
    </div>
  </div>

  <div class="col-xs-2">
    <a href="https://dritchie.github.io/">
      <img class="people-pic" src="./2021_files/daniel.png">
    </a>
    <div class="people-name">
      <a href="https://dritchie.github.io/">Daniel Ritchie</a>
      <h6>Brown University</h6>
    </div>
  </div>

  <div class="col-xs-2">
    <a href="http://msavva.github.io/">
      <img class="people-pic" src="./2021_files/manolis.png">
    </a>
    <div class="people-name">
      <a href="http://msavva.github.io/">Manolis Savva</a>
      <h6>Simon Fraser University</h6>
    </div>
  </div>

</div>

<hr>

<div class="row">
  <div class="col-xs-12">
    <h2>Prior workshops in this series</h2>
    <a href="https://learn3dgen.github.io/">CVPR 2020: Learning 3D Generative Models</a><br>
    <a href="https://3dscenegen.github.io/">CVPR 2019: 3D Scene Generation</a><br>
  </div>
</div>

<p><br>
<br></p>

<div class="row">
  <div class="col-xs-12">
    <h2>Acknowledgments</h2>
  </div>
</div>
<p><a name="/acknowledgements"></a></p>
<div class="row">
  <div class="col-xs-12">
    <p>
      Thanks to <span style="color:#1a1aff;font-weight:400;"> <a href="https://visualdialog.org/">visualdialog.org</a></span> for the webpage format.
    </p>
  </div>
</div>

<p><br></p>


      </div>
    </div>

    

    <script type="text/javascript" src="./2021_files/jquery.min.js"></script>
    <script type="text/javascript" src="./2021_files/bootstrap.min.js"></script>
    <script type="text/javascript" src="./2021_files/main.js"></script>
  

</body></html>