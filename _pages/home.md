---
layout: project
urltitle:  "Learning to Generate 3D Shapes and Scenes"
title: "Learning to Generate 3D Shapes and Scenes"
categories: cvpr, workshop, computer vision, computer graphics, deep learning, generative modeling, visual learning, simulation environments, robotics, machine learning, reinforcement learning
permalink: /
favicon: /static/img/ico/favicon.png
bibtex: true
paper: true
acknowledgements: ""
---

<br>
<div class="row">
  <div class="col-xs-12">
    <center><h1>Learning to Generate 3D Shapes and Scenes</h1></center>
    <center><h2>CVPR 2021 Workshop</h2></center>
    <center><span style="font-weight:400;">June 25th 2021 AM PDT</span></center>
    <center><span style="color:#e74c3c;font-weight:400;"></span></center>
    <br/>
  </div>
</div>

<hr>

<!-- <div class="row" id="">
  <div class="col-md-12">
    <img src="{{ "/static/img/splash.png" | prepend:site.baseurl }}">
    <p> Image credit: [1, 2, 7, 12, 6, 4, 5]</p>
  </div>
</div> -->

<!-- <b>Submit questions for the closing panel discussion using <a href=''>this Google form</a>.</b> -->

<!-- <b>Please give us your feedback on how the workshop went using <a href=''>this Google form</a>.</b> -->

<div class="row" id="intro">
  <div class="col-xs-12">
    <h2>Introduction</h2>
  </div>
</div>
<div class="row">
  <div class="col-xs-12">
    <p>
     This workshop aims to bring together researchers working on generative models of 3D shapes and scenes with researchers and practitioners who use these generative models in a variety of research areas. For our purposes, we define "generative model" to include methods that synthesize geometry unconditionally as well as from sensory inputs (e.g. images), language, or other high-level specifications. Vision tasks that can benefit from such models include scene classification and segmentation, 3D reconstruction, human activity recognition, robotic visual navigation, question answering, and more.
    </p>
  </div>
</div> <br>

<div class="row" id="cfp">
  <div class="col-xs-12">
    <h2>Call for Papers</h2>
  </div>
</div>
<div class="row">
  <div class="col-xs-12">
    <p>
      <span style="font-weight:500;">Call for papers:</span> We invite papers of up to 8 pages for work on tasks related to data-driven 3D generative modeling or tasks leveraging generated 3D content.
      Paper topics may include but are not limited to:
    </p>
    <ul>
      <li>Generative models for 3D shape and 3D scene synthesis</li>
      <li>Generating 3D shapes and scenes from real world data (images, videos, or scans)</li>
      <li>Representations for 3D shapes and scenes</li>
      <li>Completion of 3D scenes or objects in 3D scenes</li>
      <li>Unsupervised feature learning for vision tasks via 3D generative models</li>
      <li>Training data synthesis/augmentation for vision tasks via 3D generative models</li>
    </ul>
    <p>
      <span style="font-weight:500;">Submission:</span> we encourage submissions of up to 8 pages excluding references and acknowledgements.
      The submission should be in the CVPR format.
      Reviewing will be single blind.
      Accepted papers will be made publicly available as non-archival reports, allowing future submissions to archival conferences or journals.
      We welcome already published papers that are within the scope of the workshop (without re-formatting), including papers from the main CVPR conference.
      Please submit your paper to the following address by the deadline: <span style="color:#1a1aff;font-weight:400;"><a href="mailto:3dscenegeneration@gmail.com">3dscenegeneration@gmail.com</a></span>
      Please mention in your email if your submission has already been accepted for publication (and the name of the conference).
    </p>
  </div>
</div>

<div class="row" id="dates">
  <div class="col-xs-12">
    <h2>Important Dates</h2>
  </div>
</div>

<div class="row">
  <div class="col-xs-12">
    <table class="table table-striped">
      <tbody>
        <tr>
          <td>Paper Submission Deadline</td>
          <td>May 17 2021 - AoE time (UTC -12)</td>
        </tr>
        <tr>
          <td>Notification to Authors</td>
          <td>May 31 2021</td>
        </tr>
        <tr>
          <td>Camera-Ready Deadline</td>
          <td>June 7 2021</td>
        </tr>
        <tr>
          <td>Workshop Date</td>
          <td>TBD, June 19-25 2021</td>
        </tr>
      </tbody>
    </table>
  </div>
</div><br>


<div class="row" id="schedule">
  <div class="col-xs-12">
    <h2>Schedule</h2>
    <p>All times in CDT Central Daylight Time (UTC-05:00)</p>
<!--
    <p>
      In the table of events below, links labeled "Video" redirect to pre-recorded talks.
      If an event does not have a "Video" link, then it is a live session.
      After a live session finishes, a "Video" link will be added which redirects to the Zoom recording of the session.
      The links labeled "Zoom/chat" redirect to the CVPR internal webpage for the corresponding schedule item.
      These pages require a CVPR registration to access (to prevent Zoom-bombing).
    </p>
    <p>
      For links to the content for the posters and spotlight presentations, please see the "Accepted Papers" section below.
    </p>
-->
  </div>
</div>


<div class="row">
  <div class="col-xs-12">
     <table class="table table-striped">
      <tbody>
        <tr>
          <td>9:00am - 9:15am</td>
          <td>Welcome and Introduction</td>
          <td></td>
        </tr>
        <tr>
          <td>9:15am - 9:40am</td>
          <td>
          Invited Talk 1 (Ruizhen Hu)
          <br/>
          <i>Title: TBA</i>
          </td>
          <td></td>
        </tr>
        <tr>
          <td>9:40am - 10:05am</td>
          <td>
          Invited Talk 2 (S.M. Ali Eslami)
          <br/>
          <i>Title: TBA</i>
          </td>
          <td></td>
        </tr>
        <tr>
          <td>10:05am - 10:30am</td>
          <td>Invited Talk 3 (Rana Hanocka)
          <br/>
          <i>Title: TBA</i>
          </td>
          <td></td>
        </tr>
        <tr>
          <td>10:30am - 10:40am</td>
          <td>Spotlight Talks</td>
          <td></td>
        </tr>
        <tr>
          <td>10:40am - 11:25am</td>
          <td>Poster Session</td>
          <td></td>
        </tr>
        <tr>
          <td>11:25am - 11:50am</td>
          <td>
          Invited Talk 4 (Katerina Fragkiadaki)
          <br/>
          <i>Title: TBA</i>
          </td>
          <td></td>
        </tr>
        <tr>
          <td>11:50am - 12:15pm</td>
          <td>Invited Talk 5 (Roozbeh Mottaghi)
          <br/>
          <i>Title: TBA</i>
          </td>
          <td></td>
        </tr>
        <tr>
          <td>12:15pm - 1:00pm</td>
          <td>Panel Discussion (speakers & panelists)</td>
          <td></td>
        </tr>
      </tbody>
    </table>
  </div>
</div>

<!--
<br>
<div class="row" id="accepted">
  <div class="col-md-12">
    <h2>Accepted Papers</h2>
  </div>
</div>

<div class="row">
  <br/>
  <div class="col-md-12">
    <h3>Poster Session 1 (10:10am - 10:55am)</h3>
    <br/>
    <span style="font-weight:bold;"><a href='https://drive.google.com/open?id=17ogMjO6xBE8UBa3Mszeea5jzqO2pLRpG'>VoronoiNet: General Functional Approximators with Local Support</a></span><br>
    <i>Francis Williams, Jérôme Parent-Lévesque, Derek Nowrouzezahrai, Daniele Panozzo,  Kwang Moo Yi, Andrea Tagliasacchi</i>
    <br/>
    Spotlight Presentation: <a href='https://www.youtube.com/watch?v=6fBpL_sXHoE'>Video</a> | <a href='https://drive.google.com/open?id=18vSGWiKRpNkucO9K7B2YmowbiAYmcIsR'>Slides</a> | <a href='http://cvpr20.com/event/voronoinet-general-functional-approximators-with-local-support/'>Zoom/chat</a>
    <br/>
    Poster Session: <a href='https://drive.google.com/open?id=1p3NWvTzGpHVC44-cSeUqLPQhbXKPdlh0'>Poster</a> | <a href='http://cvpr20.com/event/voronoinet-general-functional-approximators-with-local-support-2/'>Zoom/chat</a>
    <br/>
    <br/>
    <span style="font-weight:bold;"><a href='https://drive.google.com/open?id=13a5EnRBaP4ElCfKimQf_3S9EVt1hHiKO'>Deep Octree-based CNNs with Output-Guided Skip Connections for 3D Shape and Scene Completion</a></span><br>
    <i>Peng-Shuai Wang, Yang Liu, Xin Tong</i>
    <br/>
    Spotlight Presentation: <a href='https://www.youtube.com/watch?v=DvKXjfKRNDs'>Video</a> | <a href='https://drive.google.com/open?id=1ouKgG4B6T7VXDq3ITGQJwaL46yhxLsO8'>Slides</a> | <a href='http://cvpr20.com/event/deep-octree-based-cnns-with-output-guided-skip-connections-for-3d-shape-and-scene-completion/'>Zoom/chat</a>
    <br/>
    Poster Session: <a href='https://drive.google.com/open?id=1IQX75T2iymTyNifDhafrkZwQk8Sw_vxU'>Poster</a> | <a href='http://cvpr20.com/event/deep-octree-based-cnns-with-output-guided-skip-connections-for-3d-shape-and-scene-completion-2/'>Zoom/chat</a>
    <br/>
    <br/>
    <span style="font-weight:bold;"><a href='https://drive.google.com/open?id=1xsAI_qIJ6nVFWXhgVvI1N2VHhOsA7Bfj'>PQ-NET: A Generative Part Seq2Seq Network for 3D Shapes</a></span><br>
    <i>Rundi Wu, Yixin Zhuang, Kai Xu, Hao Zhang, Baoquan Chen</i>
    <br/>
    Spotlight Presentation: <a href='https://www.youtube.com/watch?v=TYW0nDoLDFc'>Video</a> | <a href='https://drive.google.com/open?id=1L_oslAqgJYzFqsHmKIyeDWTqj3yYlBTG'>Slides</a> | <a href='http://cvpr20.com/event/pq-net-a-generative-part-seq2seq-network-for-3d-shapes-2/'>Zoom/chat</a>
    <br/>
    Poster Session: <a href='https://drive.google.com/open?id=125bfvxNBlDEpHF3bANMXa59bsdmU9sl2'>Poster</a> | <a href='http://cvpr20.com/event/pq-net-a-generative-part-seq2seq-network-for-3d-shapes-3/'>Zoom/chat</a>
    <br/>
    <br/>
    <span style="font-weight:bold;"><a href='https://drive.google.com/open?id=1ljzE3-m_I69uf1RJ3YYytyBdBzhjQMfJ'>Generalized Autoencoder for Volumetric Shape Generation</a></span><br>
    <i>Yanran Guan, Tansin Jahan, Oliver van Kaick</i>
    <br/>
    Spotlight Presentation: <a href='https://www.youtube.com/watch?v=s2-6mkVrHPs'>Video</a> | <a href='http://cvpr20.com/event/generalized-autoencoder-for-volumetric-shape-generation/'>Zoom/chat</a>
    <br/>
    Poster Session: <a href='https://drive.google.com/open?id=1OTMQaSUYFkAfcKVbFy4QOr9NLTQhr0uw'>Poster</a> | <a href='http://cvpr20.com/event/generalized-autoencoder-for-volumetric-shape-generation-2/'>Zoom/chat</a>
  </div>
</div>

<div class="row">
  <br/>
  <br/>
  <div class="col-md-12">
    <h3>Poster Session 2 (2:10pm - 2:55pm)</h3>
    <br/>
    <span style="font-weight:bold;"><a href='https://drive.google.com/open?id=1nyd0_3UHSUNugGc55ISTzUwBcSgzk03z'>Topology-Aware Single-Image 3D Shape Reconstruction</a></span><br>
    <i>Qimin Chen, Vincent Nguyen, Feng Han, Raimondas Kiveris, Zhuowen Tu</i>
    <br/>
    Spotlight Presentation: <a href='https://www.youtube.com/watch?v=lLdlX8oWDUI'>Video</a> | <a href='http://cvpr20.com/event/topology-aware-single-image-3d-shape-reconstruction/'>Zoom/chat</a>
    <br/>
    Poster Session: <a href='https://drive.google.com/open?id=15pXfqHKaS5LCgzWKSkL2329IwF-CmXCS'>Poster</a> | <a href='http://cvpr20.com/event/topology-aware-single-image-3d-shape-reconstruction-2/'>Zoom/chat</a>
    <br/>
    <br/>
    <span style="font-weight:bold;"><a href='https://drive.google.com/open?id=1rEbvz5zLG6NqDvGuO9x-NdCl0i62S5Fo'>Geometry to the Rescue: 3D Instance Reconstruction from a Cluttered Scene</a></span><br>
    <i>Lin Li, Salman Khan, Nick Barnes</i>
    <br/>
    Spotlight Presentation: <a href='https://www.youtube.com/watch?v=mxvCUapHZF4'>Video</a> |  <a href='http://cvpr20.com/event/geometry-to-the-rescue-3d-instance-reconstruction-from-a-cluttered-scene/'>Zoom/chat</a>
    <br/>
    Poster Session: <a href='https://drive.google.com/open?id=1e78vCO8EsEcPrkg1bSCZbKsNxc3iQflX'>Poster</a> | <a href='http://cvpr20.com/event/geometry-to-the-rescue-3d-instance-reconstruction-from-a-cluttered-scene-2/'>Zoom/chat</a>
    <br/>
    <br/>
    <span style="font-weight:bold;"><a href='https://drive.google.com/open?id=1sPLHqJ9VToHOhKPbP-VPfNvjWp-nLj10'>Mesh Variational Autoencoders with Edge Contraction Pooling</a></span><br>
    <i>Yu-Jie Yuan, Yu-Kun Lai, Jie Yang,  Qi Duan, Hongbo Fu, Lin Gao</i>
    <br/>
    Spotlight Presentation: <a href='https://www.youtube.com/watch?v=2pvh38j80jo'>Video</a> | <a href='https://drive.google.com/open?id=1wyIu_-M7N5oRkefn8m3lbSiVDr7p6iPt'>Slides</a> | <a href='http://cvpr20.com/event/mesh-variational-autoencoders-with-edge-contraction-pooling/'>Zoom/chat</a>
    <br/>
    Poster Session: <a href='https://drive.google.com/open?id=1S_gxxJDYVjmpaiRTwN917hESgwA9TLrt'>Poster</a> | <a href='http://cvpr20.com/event/mesh-variational-autoencoders-with-edge-contraction-pooling-2/'>Zoom/chat</a>
    <br/>
    <br/>
    <span style="font-weight:bold;"><a href='https://drive.google.com/open?id=1kRsNlcAmgEKHmgFuRIJBj3mjBZl3-5RH'>BSP-Net: Generating Compact Meshes via Binary Space Partitioning</a></span><br>
    <i>Zhiqin Chen, Andrea Tagliasacchi, Hao Zhang</i>
    <br/>
    Spotlight Presentation: <a href='https://www.youtube.com/watch?v=hXH1vbWG5xg'>Video</a> | <a href='https://drive.google.com/open?id=1rMraafBCV41PM_K7NMo_Y06bCbtNJKLO'>Slides</a> | <a href='http://cvpr20.com/event/bsp-net-generating-compact-meshes-via-binary-space-partitioning-2/'>Zoom/chat</a>
    <br/>
    Poster Session: <a href='https://drive.google.com/open?id=1q3Ov6ORXD5OLwVKQmuttjdWaCczN2LoO'>Poster</a> | <a href='http://cvpr20.com/event/bsp-net-generating-compact-meshes-via-binary-space-partitioning-3/'>Zoom/chat</a>
  </div>
</div>
-->

<br>
<div class="row" id="speakers">
  <div class="col-xs-12">
    <h2>Invited Speakers & Panelists</h2>
  </div>
</div><br>

<div class="row">
  <div class="col-md-12">
    <a href="https://www.cs.cmu.edu/~katef/"><img class="people-pic" style="float:left;margin-right:50px;" src="{{ "/static/img/people/katerina.png" | prepend:site.baseurl }}"></a>
    <p>
      <b><a href="https://www.cs.cmu.edu/~katef/">Katerina Fragkiadaki</a></b> is an Assistant Professor in the Machine Learning Department at Carnegie Mellon. Prior to joining MLD's faculty she worked as a post doctoral researcher first at UC Berkeley working with Jitendra Malik and then at Google Research in Mountain View working with the video group. Katerina is interested in building machines that understand the stories that videos portray, and, inversely, in using videos to teach machines about the world. The pen-ultimate goal is to build a machine that understands movie plots, and the ultimate goal is to build a machine that would want to watch Bergman over this.
    </p>
  </div>
</div><br>

<div class="row">
  <div class="col-md-12">
    <a href="http://csse.szu.edu.cn/staff/ruizhenhu/"><img class="people-pic" style="float:left;margin-right:50px;" src="{{ "/static/img/people/ruizhen.png" | prepend:site.baseurl }}"></a>
    <p>
      <b><a href="http://csse.szu.edu.cn/staff/ruizhenhu/">Ruizhen Hu</a></b> is an Associate Professor at Shenzhen University and Deputy Director of the Visual Computing Research Center (VCC). She obtained her Ph.D. degree in Applied Math under the supervision of Prof. Ligang Liu in June 2015 at Zhejiang University. She spent two years visiting the GrUVi Lab in the School of Computing Science at Simon Fraser University, under the supervision of Prof. Hao (Richard) Zhang. Her research interests are in computer graphics, with a recent focus on applying machine learning to advance the understanding and generative modeling of visual data including 3D shapes and indoor scenes.
    </p>
  </div>
</div><br>

<div class="row">
  <div class="col-md-12">
    <a href="https://www.cs.tau.ac.il/~hanocka/"><img class="people-pic" style="float:left;margin-right:50px;" src="{{ "/static/img/people/rana.png" | prepend:site.baseurl }}"></a>
    <p>
      <b><a href="https://www.cs.tau.ac.il/~hanocka/">Rana Hanocka</a></b> is a Ph.D. candidate under the supervision of Daniel Cohen-Or and Raja Giryes at Tel Aviv University. She interested in the combination of computer graphics and machine learning. Specifically, she is interested in using deep learning and exploring neural representations for manipulating, analyzing, and understanding 3D shapes.
    </p>
  </div>
</div><br>

<div class="row">
  <div class="col-md-12">
    <a href="https://roozbehm.info/"><img class="people-pic" style="float:left;margin-right:50px;" src="{{ "/static/img/people/roozbeh.png" | prepend:site.baseurl }}"></a>
    <p>
      <b><a href="https://roozbehm.info/">Roozbeh Mottaghi</a></b> is the Research Manager of the PRIOR team at Allen Institute for AI and an Affiliate Associate Professor in Paul G. Allen School of Computer  Science & Engineering at the  University  of  Washington. Prior to joining AI2, he was a post-doctoral researcher at the Computer Science Department at Stanford University. He obtained his PhD in Computer Science in 2013from University of California, Los Angeles.  His research is mainly focused on Computer Vision and Machine Learning.
    </p>
  </div>
</div><br>

<div class="row">
  <div class="col-md-12">
    <a href="http://arkitus.com/research/"><img class="people-pic" style="float:left;margin-right:50px;" src="{{ "/static/img/people/ali.png" | prepend:site.baseurl }}"></a>
    <p>
      <b><a href="http://arkitus.com/research/">S. M. Ali Eslami</a></b> is a Staff Research Scientist at Google DeepMind working on problems related to artificial intelligence. His group's research is focused on figuring out how we can get computers to learn with less supervision. Previously he was a post-doctoral researcher at Microsoft Research Cambridge. He did his PhD at the University of Edinburgh, where he was a Carnegie scholar working with Christopher Williams. During his PhD, he was also a visiting researcher at Oxford University working with Andrew Zisserman.
    </p>
  </div>
</div><br>

<div class="row">
  <div class="col-md-12">
    <a href="https://kevinkaixu.net/"><img class="people-pic" style="float:left;margin-right:50px;" src="{{ "/static/img/people/kevin.png" | prepend:site.baseurl }}"></a>
    <p>
      <b><a href="https://kevinkaixu.net/">Kai (Kevin) Xu</a></b> is an Associate Professor at the School of Computer Science, National University of Defense Technology, where he received his PhD in 2011. He conducted visiting research at Simon Fraser University (2008-2010) and Princeton University (2017-2018). His research interests include geometry processing and geometric modeling, especially on data-driven approaches to the problems in those directions, as well as 3D geometry-based vision and its robotic applications. He has co-organized several courses and tutorials on those topics at prestigious venues such as SIGGRAPH and Eurographics.
    </p>
  </div>
</div><br>

<div class="row">
  <div class="col-md-12">
    <a href="https://www.cs.utexas.edu/~huangqx/"><img class="people-pic" style="float:left;margin-right:50px;" src="{{ "/static/img/people/qixing.png" | prepend:site.baseurl }}"></a>
    <p>
      <b><a href="https://www.cs.utexas.edu/~huangqx/">Qixing Huang</a></b> is an Assistant Professor of Computer Science at the University of Texas at Austin. He obtained his PhD in Computer Science from Stanford University in 2012. From 2012 to 2014 he was a postdoctoral research scholar at Stanford University. From 2014 to 2016 he was a Research Assistant Professor at Toyota Technological Institute at Chicago. He received his MS and BS in Computer Science from Tsinghua University. He has also interned at Google Street View, Google Research and Adobe Research. His research spans computer vision, computer graphics, computational biology and machine learning.
    </p>
  </div>
</div><br>

<div class="row">
  <div class="col-md-12">
    <a href="https://www2.cs.sfu.ca/~haoz/"><img class="people-pic" style="float:left;margin-right:50px;" src="{{ "/static/img/people/richard.png" | prepend:site.baseurl }}"></a>
    <p>
      <b><a href="https://www2.cs.sfu.ca/~haoz/">Hao (Richard) Zhang</a></b> is a Distinguished University Professor at Simon Fraser University. He obtained his PhD from the University of Toronto, and M.Math and B.Math degrees from Waterloo. His research is in computer graphics with special interests in geometric modeling, shape analysis, 3D vision, geometric deep learning, as well as computational design and fabrication. He has published more than 150 papers on these topics and methods from three of his papers on geometry processing have been adopted by CGAL, the open-source Computational Geometry Algorithms Library. Awards won by Richard include an NSERC Discovery accelerator Supplement Award in 2014, a Google Faculty Research Award in 2019, as well as faculty grants/gifts from Adobe and Autodesk. He and his students have won the CVPR 2020 Best Student Paper Award and best paper awards at SGP 2008 and CAD/Graphics 2017.
    </p>
  </div>
</div><br>



<div class="row" id="organizers">
  <div class="col-xs-12">
    <h2>Organizers</h2>
  </div>
</div>

<div class="row text-center">
  <div class="col-xs-2">
    <a href="https://manyili12345.github.io/">
      <img class="people-pic" src="{{ "/static/img/people/manyi.png" | prepend:site.baseurl }}">
    </a>
    <div class="people-name">
      <a href="https://manyili12345.github.io/">Manyi Li</a>
      <h6>Simon Fraser University</h6>
    </div>
  </div>

  <div class="col-xs-2">
    <a href="https://www.cs.utexas.edu/~yzp12/">
      <img class="people-pic" src="{{ "/static/img/people/zhenpei.png" | prepend:site.baseurl }}">
    </a>
    <div class="people-name">
      <a href="https://www.cs.utexas.edu/~yzp12/">Zhenpei Yang</a>
      <h6>UT Austin</h6>
    </div>
  </div>

  <div class="col-xs-2">
    <a href="https://angelxuanchang.github.io/">
      <img class="people-pic" src="{{ "/static/img/people/angel.png" | prepend:site.baseurl }}">
    </a>
    <div class="people-name">
      <a href="https://angelxuanchang.github.io/">Angel X. Chang</a>
      <h6>Simon Fraser University</h6>
    </div>
  </div>

  <div class="col-xs-2">
    <a href="https://www.cse.iitb.ac.in/~sidch/">
      <img class="people-pic" src="{{ "/static/img/people/sid.png" | prepend:site.baseurl }}">
    </a>
    <div class="people-name">
      <a href="https://www.cse.iitb.ac.in/~sidch/">Siddhartha Chaudhuri</a>
      <h6>Adobe Research, IIT Bombay</h6>
    </div>
  </div>

  <div class="col-xs-2">
    <a href="https://dritchie.github.io/">
      <img class="people-pic" src="{{ "/static/img/people/daniel.png" | prepend:site.baseurl }}">
    </a>
    <div class="people-name">
      <a href="https://dritchie.github.io/">Daniel Ritchie</a>
      <h6>Brown University</h6>
    </div>
  </div>

  <div class="col-xs-2">
    <a href="http://msavva.github.io/">
      <img class="people-pic" src="{{ "/static/img/people/manolis.png" | prepend:site.baseurl }}">
    </a>
    <div class="people-name">
      <a href="http://msavva.github.io/">Manolis Savva</a>
      <h6>Simon Fraser University</h6>
    </div>
  </div>

</div>

<!-- <div class="row">
  <div class="col-xs-2">
    <a href="https://kevinkaixu.net/">
      <img class="people-pic" src="{{ "/static/img/people/kevin.png" | prepend:site.baseurl }}">
    </a>
    <div class="people-name">
      <a href="https://kevinkaixu.net/">Kai (Kevin) Xu</a>
      <h6>NUDT</h6>
    </div>
  </div>
  <div class="col-xs-2">
    <a href="https://www.cs.sfu.ca/~haoz/">
      <img class="people-pic" src="{{ "/static/img/people/richard.png" | prepend:site.baseurl }}">
    </a>
    <div class="people-name">
      <a href="https://www.cs.sfu.ca/~haoz/">Hao (Richard) Zhang</a>
      <h6>Simon Fraser University</h6>
    </div>
  </div>
</div> -->

<hr>

{% if page.acknowledgements %}
<div class="row">
  <div class="col-xs-12">
    <h2>Acknowledgments</h2>
  </div>
</div>
<a name="/acknowledgements"></a>
<div class="row">
  <div class="col-xs-12">
    <p>
      Thanks to <span style="color:#1a1aff;font-weight:400;"> <a href="https://visualdialog.org/">visualdialog.org</a></span> for the webpage format.
    </p>
  </div>
</div>
{% endif %}

<br>

