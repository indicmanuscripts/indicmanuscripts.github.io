---
layout: project
urltitle:  "Learning to Generate 3D Shapes and Scenes"
title: "Learning to Generate 3D Shapes and Scenes"
categories: eccv, workshop, computer vision, computer graphics, deep learning, generative modeling, visual learning, simulation environments, robotics, machine learning, reinforcement learning
permalink: /
favicon: /static/img/ico/favicon.png
bibtex: true
paper: true
acknowledgements: ""
---

<br>
<div class="row">
  <div class="col-xs-12">
    <center><h1>Learning to Generate 3D Shapes and Scenes</h1></center>
    <center><h2>ECCV 2022 Workshop</h2></center>
    <center><span style="font-weight:400;">October 23, 2022</span></center>
    <center><span style="color:#e74c3c;font-weight:400;"></span></center>
    <br/>
  </div>
</div>

<hr>

<b>Join live stream <a href="https://live.allintheloop.net/Agenda/ortra/ortraECCV2022/View_agenda/236653">here</a> (ECCV registration required).</b>

<b>Submit questions to the authors of the accepted papers: <a href="https://forms.gle/FFFVHVeTtcVkWg2n8">https://forms.gle/FFFVHVeTtcVkWg2n8</a>.</b>

<b>Submit questions for the closing panel discussion using this google form: <a href="https://forms.gle/XADQAVR8HNavtVRj6">https://forms.gle/XADQAVR8HNavtVRj6</a>.</b>


<!-- <b>Please give us your feedback on how the workshop went using this Google form: <a href='https://forms.gle/utMpEnF4hmUcR19S7'>https://forms.gle/utMpEnF4hmUcR19S7</a>.</b> -->

<div class="row" id="intro">
  <div class="col-xs-12">
    <h2>Introduction</h2>
  </div>
</div>
<div class="row">
  <div class="col-xs-12">
    <p>
     This workshop aims to bring together researchers working on generative models of 3D shapes and scenes with researchers and practitioners who use these generative models in a variety of research areas. For our purposes, we define "generative model" to include methods that synthesize geometry unconditionally as well as from sensory inputs (e.g. images), language, or other high-level specifications. Vision tasks that can benefit from such models include scene classification and segmentation, 3D reconstruction, human activity recognition, robotic visual navigation, question answering, and more.
    </p>
  </div>
</div> <br>


<div class="row" id="schedule">
  <div class="col-xs-12">
    <h2>Schedule</h2>
    <p>All times in Israel Time (UTC+03:00)</p>
  </div>
</div>


<div class="row">
  <div class="col-xs-12">
     <table class="table table-striped">
      <tbody>
        <tr>
          <td>3:00pm - 3:15pm</td>
          <td>Welcome and Introduction</td>
          <td></td>
        </tr>
        <tr>
          <td>3:15pm - 3:40pm</td>
          <td>
          Invited Talk 1 (Ruizhen Hu)
          <br/>
          <i>Title: Interaction Representation and Generation</i>
          </td>
          <td></td>
        </tr>
        <tr>
          <td>3:40pm - 4:05pm</td>
          <td>
          Invited Talk 2 (Jia Deng)
          <br/>
          <i>Title: Learning to Generate Synthetic Data</i>
          </td>
          <td></td>
        </tr>
        <tr>
          <td>4:05pm - 4:30pm</td>
          <td>Invited Talk 3 (Zhengqi Li)
          <br/>
          <i>Title: Learning Infinite View Generation from Internet Photo Collections</i>
          </td>
          <td></td>
        </tr>
        <tr>
          <td>4:30pm - 5:25pm</td>
          <td>Paper Spotlight Talks</td>
          <td></td>
        </tr>
        <!-- <tr>
          <td>11:10am - 11:25am</td>
          <td>Break</td>
          <td></td>
        </tr> -->
        <tr>
          <td>5:25pm - 5:50pm</td>
          <td>
          Invited Talk 4 (Adriana Schulz)
          <br/>
          <i>Title: Generating Content with Computer-Aided Design (CAD) Representations</i>
          </td>
          <td></td>
        </tr>
        <tr>
          <td>5:50pm - 6:15pm</td>
          <td>Invited Talk 5 (Andreas Geiger)
          <br/>
          <i>Title: Generating Images and 3D Shapes</i>
          </td>
          <td></td>
        </tr>
        <tr>
          <td>6:15pm - 7:00pm</td>
          <td>Panel Discussion (speakers & panelists)</td>
          <td></td>
        </tr>
      </tbody>
    </table>
  </div>
</div>

<div class="row" id="speakers">
  <div class="col-xs-12">
    <h2>Invited Speakers & Panelists</h2>
  </div>
</div><br>

<div class="row">
  <div class="col-md-12">
    <a href="https://www.cs.princeton.edu/~jiadeng/"><img class="people-pic" style="float:left;margin-right:50px;" src="{{ "/static/img/people/jia.jpg" | prepend:site.baseurl }}"></a>
    <p>
      <b><a href="https://www.cs.princeton.edu/~jiadeng/">Jia Deng</a></b> is an Assistant Professor of Computer Science at Princeton University. He direct the Princeton Vision & Learning Lab. His current interests include 3D vision, object recognition, action recognition, and automated theorem proving. He received his Ph.D. from Princeton University and his B.Eng. from Tsinghua University, both in computer science. He is a recipient of the Sloan Research Fellowship, the NSF CAREER award, the ONR Young Investigator award, an ICCV Marr Prize, and two ECCV Best Paper Awards.
    </p>
  </div>
</div><br>

<div class="row">
  <div class="col-md-12">
    <a href="http://www.cvlibs.net/"><img class="people-pic" style="float:left;margin-right:50px;" src="{{ "/static/img/people/andreas.jpg" | prepend:site.baseurl }}"></a>
    <p>
      <b><a href="http://www.cvlibs.net/">Andreas Geiger</a></b> is a Professor of computer science heading the Autonomous Vision Group (AVG). His group is part of the University of Tübingen and the MPI for Intelligent Systems located in Tübingen, Germany at the heart of CyberValley.  He is deputy head of the department of computer science at the University of Tübingen, PI in the cluster of excellence `ML in Science' and the CRC `Robust Vision'. He is also an ELLIS fellow, board member, and coordinator of the ELLIS PhD program. His research group is developing machine learning models for computer vision, natural language and robotics with applications in self-driving, VR/AR and scientific document analysis.
    </p>
  </div>
</div><br>

<div class="row">
  <div class="col-md-12">
    <a href="https://csse.szu.edu.cn/staff/ruizhenhu/"><img class="people-pic" style="float:left;margin-right:50px;" src="{{ "/static/img/people/ruizhen.png" | prepend:site.baseurl }}"></a>
    <p>
      <b><a href="https://csse.szu.edu.cn/staff/ruizhenhu/">Ruizhen Hu</a></b> is an Associate Professor of College of Computer Science & Software Engineering at Shenzhen University and Deputy Director of the Visual Computing Research Center (VCC). Before coming to Shenzhen Uni- versity, she was an Assistant Researcher at Shenzhen In- stitutes of Advanced Technology (SIAT). She obtained her Ph.D. degree in Applied Math under the supervision of Prof. Ligang Liu in June 2015 from the Department of Mathematics at Zhejiang University, China. From Oct.  2012 to Oct. 2014, she spent two years visiting the GrUVi Lab in the School of Computing Science at Simon Fraser University, Canada, under the supervision of Prof.  Hao (Richard) Zhang. The visit was supported by China Scholarship Council (CSC). Her research interests are in computer graphics, with a recent focus on applying machine learning to advance the understanding and generative modeling of visual data including 3D shapes and indoor scenes.
    </p>
  </div>
</div><br>

<div class="row">
  <div class="col-md-12">
    <a href="https://zhengqili.github.io/"><img class="people-pic" style="float:left;margin-right:50px;" src="{{ "/static/img/people/zhengqi.jpg" | prepend:site.baseurl }}"></a>
    <p>
      <b><a href="https://zhengqili.github.io/">Zhengqi Li</a></b> is a research scientist at Google Research. His research interests span 3D/4D computer vision, epsically for images and videos in the wild. He received his CS Ph.D. degree at Cornell University where he was advised by Prof. Noah Snavely. He received his Bachelor of Computer Engineering with High Distinction at University of Minnesota, Twin Cities where he was advised by Prof. Stergios Roumeliotis and was a research assistant at MARS Lab and Google Project Tango (now ARCore). He was also a member of Robotic Sensor Networks (RSN) Lab where he worked closely with Prof. Volkan Isler. He is a recipient of the CVPR 2019 Best Paper Hornorable Mention, 2020 Google Ph.D. Fellowship, 2020 Adobe Research Fellowship, and 2021 Baidu Global Top 100 Chinese Rising Stars in AI.
    </p>
  </div>
</div><br>

<div class="row">
  <div class="col-md-12">
    <a href="https://homes.cs.washington.edu/~adriana/"><img class="people-pic" style="float:left;margin-right:50px;" src="{{ "/static/img/people/adriana.jpg" | prepend:site.baseurl }}"></a>
    <p>
      <b><a href="https://homes.cs.washington.edu/~adriana/">Adriana Schulz</a></b> is an assistant professor at the Paul G. Allen School of Computer Science & Engineering at the University of Washington and a member of the Computer Graphics Group (GRAIL). She is also co-director of the Digital Fabrication Center at UW (DFab) and the director of WiGRAPH. Her research group creates manufacturing design systems that will revolutionize how we build physical artifacts, and builds next-generation design tools for manufacturing that fundamentally change what can be made, and by whom. 
    </p>
  </div>
</div><br>

<div class="row" id="cfp">
  <div class="col-xs-12">
    <h2>Call for Papers</h2>
  </div>
</div>
<div class="row">
  <div class="col-xs-12">
    <p>
      <span style="font-weight:500;">Call for papers:</span> We invite papers of up to 14 pages for work on tasks related to data-driven 3D generative modeling or tasks leveraging generated 3D content.
      Paper topics may include but are not limited to:
    </p>
    <ul>
      <li>Generative models for 3D shape and 3D scene synthesis</li>
      <li>Generating 3D shapes and scenes from real world data (images, videos, or scans)</li>
      <li>Representations for 3D shapes and scenes</li>
      <li>Completion of 3D scenes or objects in 3D scenes</li>
      <li>Unsupervised feature learning for vision tasks via 3D generative models</li>
      <li>Training data synthesis/augmentation for vision tasks via 3D generative models</li>
    </ul>
    <p>
      <span style="font-weight:500;">Submission:</span> we encourage submissions of up to 14 pages excluding references and acknowledgements.
      The submission should be in the ECCV format.
      Reviewing will be single blind.
      Accepted papers will be made publicly available as non-archival reports, allowing future submissions to archival conferences or journals.
      We welcome already published papers that are within the scope of the workshop (without re-formatting), including papers from the main ECCV conference.
      Please submit your paper to the following address by the deadline: <span style="color:#1a1aff;font-weight:400;"><a href="mailto:3dscenegeneration@gmail.com">3dscenegeneration@gmail.com</a></span>
      Please mention in your email if your submission has already been accepted for publication (and the name of the conference).
    </p>
  </div>
</div>

<div class="row" id="dates">
  <div class="col-xs-12">
    <h2>Important Dates</h2>
  </div>
</div>

<div class="row">
  <div class="col-xs-12">
    <table class="table table-striped">
      <tbody>
        <tr>
          <td>Paper Submission Deadline</td>
          <td>Sep 19 2022 - AoE time (UTC -12)</td>
        </tr>
        <tr>
          <td>Notification to Authors</td>
          <td>Sep 26 2022</td>
        </tr>
        <tr>
          <td>Camera-Ready Deadline</td>
          <td>Oct 5 2022</td>
        </tr>
        <tr>
          <td>Workshop Date</td>
          <td>Oct 23 2022</td>
        </tr>
      </tbody>
    </table>
  </div>
</div>

<br>
<div class="row" id="accepted">
  <div class="col-md-12">
    <h2>Accepted Papers</h2>
  </div>
</div>

<div class="row text-center">
  <div class="col-md-12">
    <hr>
    <br/>
    <span style="font-weight:bold;">
    <img src="{{ "/static/img/poster/0001-poster.png" | prepend:site.baseurl }}" width='700'><br/>
    <a href=''>3D GAN Inversion for Controllable Portrait Image Animation</a></span>
    <br/>
    <i>Connor Z. Lin, David B. Lindell, Eric R. Chan, Gordon Wetzstein</i>
    <br/>
    <a href='https://drive.google.com/file/d/18qcR7WjImq_wRpfLf2aI7cRb62JyMppY/view?usp=sharing'>Paper</a> | <a href='https://drive.google.com/file/d/1ZEHbONpIk8tGnCY3PWB3gpw-Cqf-_fLt/view?usp=sharing'>Poster</a> | <a href='https://drive.google.com/file/d/1HkDUt1z7M_Bv5THurz8-wewoqqkg7HFo/view?usp=sharing'>Video</a>
    <br/><hr>
    <br/>
    <span style="font-weight:bold;">
    <img src="{{ "/static/img/poster/0002-poster.png" | prepend:site.baseurl }}" width='700'><br/>
    <a href=''>3DLatNav: Navigating Generative Latent Spaces for Semantic-Aware 3D Object Manipulation</a></span>
    <br/>
    <i>Amaya Dharmasiri, Dinithi Dissanayake, Mohamed Afham, Isuru Dissanayake, Ranga Rodrigo, Kanchana Thilakarathna</i>
    <br/>
    <a href='https://drive.google.com/file/d/1ZnObAPLwCYvUCqyreMr7dWOlWEvcEL4W/view?usp=sharing'>Paper</a> | <a href='https://drive.google.com/file/d/1rD_YGMNfUkVA_7-RJ6SX8y_rjPZYZlbU/view?usp=sharing'>Poster</a> | <a href='https://drive.google.com/file/d/1-NMo44QMzLzYM2Mh7og2P3Xa8MgU8eyE/view?usp=sharing'>Video</a>
    <br/><hr>
    <br/>
    <span style="font-weight:bold;">
    <img src="{{ "/static/img/poster/0003-poster.png" | prepend:site.baseurl }}" width='700'><br/>
    <a href=''>3D Semantic Label Transfer and Matching in Human-Robot Collaboration</a></span>
    <br/>
    <i>Szilvia Szeier, Benjámin Baffy, Gábor Baranyi, Joul Skaf, László Kopácsi, Daniel Sonntag, Gábor Sörös, and András Lőrincz</i>
    <br/>
    <a href='https://drive.google.com/file/d/1GD065M4qj2BhT6ujZEv_kMTFTmBYWL6C/view?usp=sharing'>Paper</a> | <a href='https://drive.google.com/file/d/15GsMVDqnVBQnmg-bIifgY8gENf-xtAn0/view?usp=sharing'>Poster</a> | <a href='https://drive.google.com/file/d/1-NMo44QMzLzYM2Mh7og2P3Xa8MgU8eyE/view?usp=sharing'>Video</a>
    <br/><hr>
    <br/>
    <span style="font-weight:bold;">
    <img src="{{ "/static/img/poster/0004-poster.png" | prepend:site.baseurl }}" width='700'><br/>
    <a href=''>Generative Multiplane Images: Making a 2D GAN 3D-Aware</a></span>
    <br/>
    <i>Xiaoming Zhao, Fangchang Ma, David Güera, Zhile Ren, Alexander G. Schwing, Alex Colburn</i>
    <br/>
    <a href='https://drive.google.com/file/d/13OVLhihZ5xQEuY_tTu94fZTu_U7WXOpO/view?usp=sharing'>Paper</a> | <a href='https://drive.google.com/file/d/1H4X3FnV2GLhdxc4jJQD45T1EweLb3lZC/view?usp=sharing'>Poster</a> | <a href='https://drive.google.com/file/d/1-NMo44QMzLzYM2Mh7og2P3Xa8MgU8eyE/view?usp=sharing'>Video</a>
    <br/><hr>
    <br/>
    <span style="font-weight:bold;">
    <img src="{{ "/static/img/poster/0005-poster.png" | prepend:site.baseurl }}" width='700'><br/>
    <a href=''>Intrinsic Neural Fields: Learning Functions on Manifolds</a></span>
    <br/>
    <i>Lukas Koestler, Daniel Grittner, Michael Moeller, Daniel Cremers, Zorah Lähner</i>
    <br/>
    <a href='https://drive.google.com/file/d/1gfRpZL1HzAkyAVqnD-bWjWTPSNtBdlhk/view?usp=sharing'>Paper</a> | <a href='https://drive.google.com/file/d/19g5Nq3YIg8KdEkG77QN0QQpORgVeHH39/view?usp=sharing'>Poster</a> | <a href='https://drive.google.com/file/d/1-NMo44QMzLzYM2Mh7og2P3Xa8MgU8eyE/view?usp=sharing'>Video</a>
    <br/><hr>
    <br/>
    <span style="font-weight:bold;">
    <img src="{{ "/static/img/poster/0006-poster.png" | prepend:site.baseurl }}" width='700'><br/>
    <a href=''>Learning Joint Surface Atlases</a></span>
    <br/>
    <i>Theo Deprelle, Thibault Groueix, Noam Aigerman, Vladimir G. Kim, Mathieu Aubry</i>
    <br/>
    <a href='https://drive.google.com/file/d/1udFGRnASw9iLDf7PyKMCr_-oZOkU9msR/view?usp=sharing'>Paper</a> | <a href='https://drive.google.com/file/d/1xlEOAWprb1TDx54MNKCVA-lx3uWXDvBn/view?usp=sharing'>Poster</a> | <a href='https://drive.google.com/file/d/1-NMo44QMzLzYM2Mh7og2P3Xa8MgU8eyE/view?usp=sharing'>Video</a>
    <br/><hr>
    <br/>
    <span style="font-weight:bold;">
    <img src="{{ "/static/img/poster/0007-poster.png" | prepend:site.baseurl }}" width='700'><br/>
    <a href=''>Learning Neural Radiance Fields from Multi-View Geometry</a></span>
    <br/>
    <i>Marco Orsingher, Paolo Zani, Paolo Medici, Massimo Bertozzi</i>
    <br/>
    <a href='https://drive.google.com/file/d/1iVmNqUEzmPrHsimYqfncO-rxpt7T-ekX/view?usp=sharing'>Paper</a> | <a href='https://drive.google.com/file/d/195vUJdaeFqCqprm6to6s_NEfo6pacMhm/view?usp=sharing'>Poster</a> | <a href='https://drive.google.com/file/d/1-NMo44QMzLzYM2Mh7og2P3Xa8MgU8eyE/view?usp=sharing'>Video</a>
    <br/><hr>
    <br/>
    <span style="font-weight:bold;">
    <img src="{{ "/static/img/poster/0008-poster.png" | prepend:site.baseurl }}" width='700'><br/>
    <a href=''>Mosaic-based omnidirectional depth estimation for view synthesis</a></span>
    <br/>
    <i>Min-jung Shin, Minji Cho, Woojune Park, Kyeongbo Kong, Joonsoo Kim, Kug-jin Yun, Gwangsoon Lee, Suk-Ju Kang</i>
    <br/>
    <a href='https://drive.google.com/file/d/1o8lQ35W7DsVWQuCsjHmRwscPe3qHX2gx/view?usp=sharing'>Paper</a> | <a href='https://drive.google.com/file/d/16LQyXp6cMG5ttMCjOKUpUUxLScdQlhPU/view?usp=sharing'>Poster</a> | <a href='https://drive.google.com/file/d/1-NMo44QMzLzYM2Mh7og2P3Xa8MgU8eyE/view?usp=sharing'>Video</a>
    <br/><hr>
    <br/>
    <span style="font-weight:bold;">
    <img src="{{ "/static/img/poster/0009-poster.png" | prepend:site.baseurl }}" width='700'><br/>
    <a href=''>Neural Shape Compiler: A Unified Framework for Transforming between Text, Point Cloud, and Program</a></span>
    <br/>
    <i>Tiange Luo, Honglak Lee, Justin Johnson</i>
    <br/>
    <a href='https://drive.google.com/file/d/16LQyXp6cMG5ttMCjOKUpUUxLScdQlhPU/view?usp=sharing'>Paper</a> | <a href='https://drive.google.com/file/d/1drW8odKGHqiZ-5Hykh6f2TsHveF8buO_/view?usp=sharing'>Poster</a> | <a href='https://drive.google.com/file/d/1-NMo44QMzLzYM2Mh7og2P3Xa8MgU8eyE/view?usp=sharing'>Video</a>
    <br/><hr>
    <br/>
    <span style="font-weight:bold;">
    <img src="{{ "/static/img/poster/0010-poster.png" | prepend:site.baseurl }}" width='700'><br/>
    <a href=''>RTMV: A Ray-Traced Multi-View Synthetic Dataset for Novel View Synthesis</a></span>
    <br/>
    <i>Jonathan Tremblay, Moustafa Meshry, Alex Evans, Jan Kautz, Alexander Keller, Sameh Khamis, Thomas Müeller, Charles Loop, Nathan Morrica, Koki Nagano, Towaki Takikawa, Stan Birchfield</i>
    <br/>
    <a href='https://drive.google.com/file/d/1SzKp_SD4-vyabtuo5RxMro2P6uZlWDyl/view?usp=sharing'>Paper</a> | <a href='https://drive.google.com/file/d/1fVA7aTR1XbtfTepEvPSc79Zt-5lupzMt/view?usp=sharing'>Poster</a> | <a href='https://drive.google.com/file/d/1-NMo44QMzLzYM2Mh7og2P3Xa8MgU8eyE/view?usp=sharing'>Video</a>
    <br/><hr>
    <br/>
    <span style="font-weight:bold;">
    <img src="{{ "/static/img/poster/0011-poster.png" | prepend:site.baseurl }}" width='700'><br/>
    <a href=''>Recovering Detail in 3D Shapes Using Disparity Maps</a></span>
    <br/>
    <i>Marissa Ramirez de Chanlatte, Matheus Gadelha, Thibault Groueix, Radomir Mech</i>
    <br/>
    <a href='https://drive.google.com/file/d/1tZKknBI4iTdBJ_9lhZIY8nESDVDDPJFu/view?usp=sharing'>Paper</a> | <a href='https://drive.google.com/file/d/15Lf7ki5o4f3YA7n6PXgfzQG5zxkJ3VNF/view?usp=sharing'>Poster</a> | <a href='https://drive.google.com/file/d/1-NMo44QMzLzYM2Mh7og2P3Xa8MgU8eyE/view?usp=sharing'>Video</a>
    <br/><hr>
    <br/>
    <span style="font-weight:bold;">
    <img src="{{ "/static/img/poster/0012-poster.png" | prepend:site.baseurl }}" width='700'><br/>
    <a href=''>Share With Thy Neighbors: Single-View Reconstruction by Cross-Instance Consistency</a></span>
    <br/>
    <i>Tom Monnier, Matthew Fisher, Alexei A. Efros, Mathieu Aubry</i>
    <br/>
    <a href='https://drive.google.com/file/d/116Ou7tuDWk6oOPaw-ng4PCM9mMz84jn-/view?usp=sharing'>Paper</a> | <a href='https://drive.google.com/file/d/15iUfNkT7dr8E2fuqkKECGhMalXQg87U2/view?usp=sharing'>Poster</a> | <a href='https://drive.google.com/file/d/1-NMo44QMzLzYM2Mh7og2P3Xa8MgU8eyE/view?usp=sharing'>Video</a>
    <br/><hr>
    <br/>
    <span style="font-weight:bold;">
    <img src="{{ "/static/img/poster/0013-poster.png" | prepend:site.baseurl }}" width='700'><br/>
    <a href=''>Towards Generalising Neural Implicit Representations</a></span>
    <br/>
    <i>Theo W. Costain, Victor A. Prisacariu</i>
    <br/>
    <a href='https://drive.google.com/file/d/15iUfNkT7dr8E2fuqkKECGhMalXQg87U2/view?usp=sharing'>Paper</a> | <a href='https://drive.google.com/file/d/15iUfNkT7dr8E2fuqkKECGhMalXQg87U2/view?usp=sharing'>Poster</a> | <a href='https://drive.google.com/file/d/1-NMo44QMzLzYM2Mh7og2P3Xa8MgU8eyE/view?usp=sharing'>Video</a>
    <br/><hr>
  </div>
</div>

<div class="row" id="organizers">
  <div class="col-xs-12">
    <h2>Organizers</h2>
  </div>
</div>

<div class="row text-center">
  <div class="col-xs-2">
    <a href="https://kwang-ether.github.io/">
      <img class="people-pic" src="{{ "/static/img/people/kai.jpg" | prepend:site.baseurl }}">
    </a>
    <div class="people-name">
      <a href="https://kwang-ether.github.io/">Kai Wang</a>
      <h6>Brown University</h6>
    </div>
  </div>

  <div class="col-xs-2">
    <a href="https://sfu.ca/~agadipat">
      <img class="people-pic" src="{{ "/static/img/people/akshay.jpg" | prepend:site.baseurl }}">
    </a>
    <div class="people-name">
      <a href="https://sfu.ca/~agadipat">Akshay Gadi Patil</a>
      <h6>Simon Fraser University</h6>
    </div>
  </div>

  <div class="col-xs-2">
    <a href="https://angelxuanchang.github.io/">
      <img class="people-pic" src="{{ "/static/img/people/angel.png" | prepend:site.baseurl }}">
    </a>
    <div class="people-name">
      <a href="https://angelxuanchang.github.io/">Angel X. Chang</a>
      <h6>Simon Fraser University</h6>
    </div>
  </div>

  <div class="col-xs-2">
    <a href="https://paulguerrero.net">
      <img class="people-pic" src="{{ "/static/img/people/paul.png" | prepend:site.baseurl }}">
    </a>
    <div class="people-name">
      <a href="https://paulguerrero.net">Paul Guerrero</a>
      <h6>Adobe Research, London</h6>
    </div>
  </div>

  <div class="col-xs-2">
    <a href="https://dritchie.github.io/">
      <img class="people-pic" src="{{ "/static/img/people/daniel.png" | prepend:site.baseurl }}">
    </a>
    <div class="people-name">
      <a href="https://dritchie.github.io/">Daniel Ritchie</a>
      <h6>Brown University</h6>
    </div>
  </div>

  <div class="col-xs-2">
    <a href="http://msavva.github.io/">
      <img class="people-pic" src="{{ "/static/img/people/manolis.png" | prepend:site.baseurl }}">
    </a>
    <div class="people-name">
      <a href="http://msavva.github.io/">Manolis Savva</a>
      <h6>Simon Fraser University</h6>
    </div>
  </div>

</div>

<hr>

<div class="row">
  <div class="col-xs-12">
    <h2>Prior workshops in this series</h2>
    <a href="static/2021.html">CVPR 2021: Learning to Generate 3D Shapes and Scenes</a><br/>
    <a href="https://learn3dgen.github.io/">CVPR 2020: Learning 3D Generative Models</a><br/>
    <a href="https://3dscenegen.github.io/">CVPR 2019: 3D Scene Generation</a><br/>
  </div>
</div>

<br/>
<br/>

{% if page.acknowledgements %}
<div class="row">
  <div class="col-xs-12">
    <h2>Acknowledgments</h2>
  </div>
</div>
<a name="/acknowledgements"></a>
<div class="row">
  <div class="col-xs-12">
    <p>
      Thanks to <span style="color:#1a1aff;font-weight:400;"> <a href="https://visualdialog.org/">visualdialog.org</a></span> for the webpage format.
    </p>
  </div>
</div>
{% endif %}

<br>

